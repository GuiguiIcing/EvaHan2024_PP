[Data]
base_model = 'SIKU-BERT/'

[Network]
n_embed = 100
n_char_embed = 50
n_feat_embed = 100
n_bert_layers = 4
embed_dropout = .33
n_lstm_hidden = 400
n_lstm_layers = 3
lstm_dropout = .33
n_mlp_span = 500
n_mlp_label = 100
n_mlp = 500
mlp_dropout = .33

[Optimizer]
bert_lr = 2e-5
mu = .9
nu = .9
epsilon = 1e-12
clip = 5.0
gpt_decay = .1
bert_decay = 0.01
decay_epochs = 45

[Run]
batch_size = 128
epochs = 200
patience = 10
